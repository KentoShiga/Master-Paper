{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0916b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import cv2\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial import ConvexHull\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyVHR as vhr\n",
    "from pyVHR.extraction.sig_processing import SignalProcessing\n",
    "from pyVHR.plot.visualize import *\n",
    "from pyVHR.BVP import *\n",
    "vhr.plot.VisualizeParams.renderer = 'notebook'\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# 修正: scikit-image 0.18.3用のインポート\n",
    "from skimage.feature import local_binary_pattern\n",
    "# graycomatrixとgraycopropsは古いバージョンでは別の場所にあります\n",
    "try:\n",
    "    from skimage.feature import greycomatrix as graycomatrix, greycoprops as graycoprops\n",
    "except ImportError:\n",
    "    # 別の方法を試す\n",
    "    from skimage.feature import texture\n",
    "    # または関数を使わない場合はコメントアウト\n",
    "    graycomatrix = None\n",
    "    graycoprops = None\n",
    "    print(\"警告: graycomatrix/graycopropsをインポートできませんでした\")\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from scipy.ndimage import laplace\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力とする動画と動画のファイル名を取得\n",
    "root_dir = \"experimentData\\\\\"\n",
    "data_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "movie_paths = []\n",
    "movie_names = []\n",
    "true_value_csv_array = []\n",
    "true_value_rri_csv_array = []\n",
    "print(\"動画ディレクトリ:\", data_dirs)\n",
    "\n",
    "for i in range(len(data_dirs)):\n",
    "    data_dir = data_dirs[i]\n",
    "\n",
    "    # 動画ファイルのパスを取得\n",
    "    movie_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.avi')]\n",
    "    movie_paths.extend(movie_files)\n",
    "\n",
    "    movie_name = os.path.basename(data_dir)\n",
    "    movie_names.append(movie_name)\n",
    "\n",
    "    # ppgファイルのパスを取得\n",
    "    movie_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    true_value_csv_array = [f.replace('.avi', '.csv') for f in movie_paths if f.endswith('.avi')]\n",
    "    true_value_rri_csv_array.append(os.path.join(data_dir, 'RRI_Simple_' + movie_name + '.csv'))\n",
    "\n",
    "\n",
    "f_1_ffi = 0.0399  # LFのはじめ\n",
    "f_2 = 0.151  # LFの終わり、HFのはじめ\n",
    "f_3 = 0.401  # HFの終わり\n",
    "\n",
    "start_index = 0\n",
    "end_index = len(movie_paths)\n",
    "end_index = 1\n",
    "\n",
    "data_dirs = data_dirs[start_index:end_index]\n",
    "movie_paths = movie_paths[start_index:end_index]\n",
    "movie_names = movie_names[start_index:end_index]\n",
    "true_value_csv_array = true_value_csv_array[start_index:end_index]\n",
    "true_value_rri_csv_array = true_value_rri_csv_array[start_index:end_index]\n",
    "\n",
    "print(f\"data_dirs: {data_dirs}\")\n",
    "print(f\"movie_paths: {movie_paths}\")\n",
    "print(f\"movie_names: {movie_names}\")\n",
    "print(f\"true_value_csv_array: {true_value_csv_array}\")\n",
    "print(f\"true_value_rri_csv_array: {true_value_rri_csv_array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddb937",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR  = \"LandmarksAccuracyEvalResults\"\n",
    "# rPPGに最適なランドマーク番号を定義\n",
    "\n",
    "# 代表的な部分のみ\n",
    "# FOREHEAD_LANDMARKS = [151]  # 額\n",
    "UPPER_LEFT_CHEEK_LANDMARKS = [119]  # 左頬\n",
    "LOWER_LEFT_CHEEK_LANDMARKS = [207]  # 左頬\n",
    "UPPER_RIGHT_CHEEK_LANDMARKS = [347]  # 右頬\n",
    "LOWER_RIGHT_CHEEK_LANDMARKS = [425]  # 右頬\n",
    "NOSE_LANDMARKS = [4]\n",
    "CHIN_LANDMARKS = [200]  # 顎\n",
    "\n",
    "# ALL_RPPG_LANDMARKS = FOREHEAD_LANDMARKS + UPPER_LEFT_CHEEK_LANDMARKS + LOWER_LEFT_CHEEK_LANDMARKS + UPPER_RIGHT_CHEEK_LANDMARKS + LOWER_RIGHT_CHEEK_LANDMARKS + NOSE_LANDMARKS + CHIN_LANDMARKS\n",
    "ALL_RPPG_LANDMARKS = UPPER_LEFT_CHEEK_LANDMARKS + LOWER_LEFT_CHEEK_LANDMARKS + UPPER_RIGHT_CHEEK_LANDMARKS + LOWER_RIGHT_CHEEK_LANDMARKS + NOSE_LANDMARKS + CHIN_LANDMARKS\n",
    "# PATCH_SIZE = 10.0  # ランドマーク周辺のBoxelの一辺の長さ（ピクセル数）\n",
    "PATCH_SIZE = 30.0  # ランドマーク周辺のBoxelの一辺の長さ（ピクセル数）\n",
    "# PATCH_SIZE = 50.0  # ランドマーク周辺のBoxelの一辺の長さ（ピクセル数）\n",
    "\n",
    "LANDMARK_ID_NAME_MAP = {\n",
    "    151: \"Forehead\",\n",
    "    119: \"Upper Left Cheek\",\n",
    "    207: \"Lower Left Cheek\",\n",
    "    347: \"Upper Right Cheek\",\n",
    "    425: \"Lower Right Cheek\",\n",
    "    4: \"Nose\",\n",
    "    200: \"Chin\"\n",
    "}\n",
    "\n",
    "GLOBAL_SAVE_DIR = root_dir\n",
    "print(f\"GLOBAL_SAVE_DIR: {GLOBAL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273c039",
   "metadata": {},
   "source": [
    "ランドマークの座標を記録\n",
    "- {dataName}_landmarks_coordinates.csvを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie_idx in range(len(movie_paths)):\n",
    "    inputMoviePath = movie_paths[movie_idx]\n",
    "    rootDir = data_dirs[movie_idx]\n",
    "    dataName = movie_names[movie_idx]\n",
    "    saveDir = os.path.join(rootDir, SAVE_DIR)\n",
    "    print(f\"ランドマーク抽出結果保存ディレクトリ: {saveDir}\")\n",
    "    os.makedirs(saveDir, exist_ok=True)\n",
    "    print(f'Processing movie: {inputMoviePath}')\n",
    "    \n",
    "    # 動画のfpsを取得\n",
    "    cap = cv2.VideoCapture(inputMoviePath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    samplingRate = fps\n",
    "    \n",
    "    # 1フレーム目を読み込む\n",
    "    ret, first_frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(f\"Error: 最初のフレームを読み込めませんでした: {inputMoviePath}\")\n",
    "        continue\n",
    "    \n",
    "    # === Step 1: ランドマーク抽出用のSignalProcessing初期化 ===\n",
    "    print(\"=== ランドマーク抽出開始 ===\")\n",
    "    sig_processing = SignalProcessing()\n",
    "    sig_processing.set_landmarks(ALL_RPPG_LANDMARKS)\n",
    "    sig_processing.set_square_patches_side(PATCH_SIZE)\n",
    "    \n",
    "    # 可視化を有効化\n",
    "    sig_processing.set_visualize_skin_and_landmarks(\n",
    "        visualize_skin=True,\n",
    "        visualize_landmarks=True,\n",
    "        visualize_landmarks_number=True,\n",
    "        visualize_patch=True\n",
    "    )\n",
    "    \n",
    "    # 肌抽出器を設定(ConvexHullを使用)\n",
    "    sig_processing.set_skin_extractor(vhr.extraction.SkinExtractionConvexHull('CPU'))\n",
    "    \n",
    "    # 1フレームのみ処理\n",
    "    sig_processing.set_total_frames(1)\n",
    "    \n",
    "    # ダミーでextract_patchesを実行してランドマーク検出を行う\n",
    "    try:\n",
    "        _ = sig_processing.extract_patches(inputMoviePath, \"squares\", \"mean\")\n",
    "        \n",
    "        # 可視化画像を取得\n",
    "        visualize_patches = sig_processing.get_visualize_patches()\n",
    "        \n",
    "        if len(visualize_patches) > 0:\n",
    "            # 1フレーム目の可視化画像を保存\n",
    "            output_path = os.path.join(saveDir, f\"{dataName}_landmarks_frame1.png\")\n",
    "            cv2.imwrite(output_path, visualize_patches[0])\n",
    "            print(f\"ランドマーク画像を保存しました: {output_path}\")\n",
    "            \n",
    "            # ランドマークの座標を取得して保存\n",
    "            # MediaPipeを使用してランドマークを検出\n",
    "            import mediapipe as mp\n",
    "            mp_face_mesh = mp.solutions.face_mesh\n",
    "            \n",
    "            with mp_face_mesh.FaceMesh(\n",
    "                static_image_mode=True,\n",
    "                max_num_faces=1,\n",
    "                refine_landmarks=True,\n",
    "                min_detection_confidence=0.5) as face_mesh:\n",
    "                \n",
    "                # RGB変換\n",
    "                frame_rgb = cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_mesh.process(frame_rgb)\n",
    "                \n",
    "                if results.multi_face_landmarks:\n",
    "                    landmarks_data = []\n",
    "                    face_landmarks = results.multi_face_landmarks[0]\n",
    "                    h, w = first_frame.shape[:2]\n",
    "                    \n",
    "                    # 使用したランドマークの座標を取得\n",
    "                    for idx in ALL_RPPG_LANDMARKS:\n",
    "                        landmark = face_landmarks.landmark[idx]\n",
    "                        x = int(landmark.x * w)\n",
    "                        y = int(landmark.y * h)\n",
    "                        landmarks_data.append({\n",
    "                            'landmark_id': idx,\n",
    "                            'x': x,\n",
    "                            'y': y\n",
    "                        })\n",
    "                    \n",
    "                    # CSVファイルに保存\n",
    "                    import pandas as pd\n",
    "                    df = pd.DataFrame(landmarks_data)\n",
    "                    csv_path = os.path.join(saveDir, f\"{dataName}_landmarks_coordinates.csv\")\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"ランドマーク座標を保存しました: {csv_path}\")\n",
    "                else:\n",
    "                    print(f\"Warning: 顔のランドマークが検出されませんでした: {inputMoviePath}\")\n",
    "        else:\n",
    "            print(f\"Warning: ランドマークが検出されませんでした: {inputMoviePath}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: ランドマーク検出中にエラーが発生しました: {e}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"=== {dataName} の処理完了 ===\\n\")\n",
    "\n",
    "    # ここにランドマークのidと座標を保存するコードを追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5424f67",
   "metadata": {},
   "source": [
    "記録された座標を読み込み、各々のRGBを記録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe737fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_df(video_path, roi_coordinates, fps):\n",
    "    \"\"\"\n",
    "    RGB + HSV + L*a*b*特徴量を含む画像品質解析を行う関数\n",
    "    \n",
    "    Args:\n",
    "        video_path: 動画ファイルのパス\n",
    "        roi_coordinates: ROI座標 (left, top, right, bottom)\n",
    "        fps: フレームレート\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 解析結果\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # 結果を格納するリスト\n",
    "    results = {\n",
    "        'frame_number': [],\n",
    "        'timestamp': [],\n",
    "        'contrast': [],\n",
    "        'r_mean': [],\n",
    "        'g_mean': [],\n",
    "        'b_mean': [],\n",
    "        'r_std': [],\n",
    "        'g_std': [],\n",
    "        'b_std': [],\n",
    "        's_mean': [],\n",
    "        's_std': [],\n",
    "        'l_mean': [],\n",
    "        'l_std': [],\n",
    "    }\n",
    "    \n",
    "    frame_count = 0\n",
    "    roi_left, roi_top, roi_right, roi_bottom = roi_coordinates\n",
    "    \n",
    "    print(f\"動画解析開始: {video_path}\")\n",
    "    print(f\"ROI座標: ({roi_left}, {roi_top}, {roi_right}, {roi_bottom})\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # ROI領域を切り出し\n",
    "        roi_frame = frame[roi_top:roi_bottom, roi_left:roi_right]\n",
    "        \n",
    "        if roi_frame.size == 0:\n",
    "            continue\n",
    "            \n",
    "        # タイムスタンプ計算\n",
    "        timestamp = frame_count / fps\n",
    "        \n",
    "        # RGB解析\n",
    "        b_channel, g_channel, r_channel = cv2.split(roi_frame)\n",
    "        \n",
    "        r_mean = np.mean(r_channel)\n",
    "        g_mean = np.mean(g_channel)\n",
    "        b_mean = np.mean(b_channel)\n",
    "        \n",
    "        r_std = np.std(r_channel)\n",
    "        g_std = np.std(g_channel)\n",
    "        b_std = np.std(b_channel)\n",
    "        \n",
    "        # コントラスト計算（グレースケール）\n",
    "        gray_roi = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "        contrast = np.std(gray_roi)\n",
    "        \n",
    "        # HSV解析\n",
    "        hsv_roi = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2HSV)\n",
    "        h_channel, s_channel, v_channel = cv2.split(hsv_roi)\n",
    "        \n",
    "        s_mean = np.mean(s_channel)\n",
    "        s_std = np.std(s_channel)\n",
    "        \n",
    "        # L*a*b*解析\n",
    "        lab_roi = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2LAB)\n",
    "        l_channel, a_channel, b_channel_lab = cv2.split(lab_roi)\n",
    "        \n",
    "        l_mean = np.mean(l_channel)\n",
    "        l_std = np.std(l_channel)\n",
    "        \n",
    "        # 結果を保存\n",
    "        results['frame_number'].append(frame_count)\n",
    "        results['timestamp'].append(timestamp)\n",
    "        results['contrast'].append(contrast)\n",
    "        results['r_mean'].append(r_mean)\n",
    "        results['g_mean'].append(g_mean)\n",
    "        results['b_mean'].append(b_mean)\n",
    "        results['r_std'].append(r_std)\n",
    "        results['g_std'].append(g_std)\n",
    "        results['b_std'].append(b_std)\n",
    "        results['s_mean'].append(s_mean)\n",
    "        results['s_std'].append(s_std)\n",
    "        results['l_mean'].append(l_mean)\n",
    "        results['l_std'].append(l_std)\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # 進捗表示\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"処理済みフレーム: {frame_count}\")\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # DataFrameに変換\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"解析完了: {len(df)} フレーム処理\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie_idx in range(len(movie_paths)):  # ★ 変数名を変更\n",
    "    inputMoviePath = movie_paths[movie_idx]\n",
    "    rootDir = data_dirs[movie_idx]\n",
    "    dataName = movie_names[movie_idx]\n",
    "    \n",
    "    print(f'Processing movie: {dataName}')  # ★ デバッグ用\n",
    "    \n",
    "    cap = cv2.VideoCapture(inputMoviePath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    savedir = os.path.join(rootDir, SAVE_DIR)\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    \n",
    "    # ROIの座標を取得\n",
    "    roi_coordinates_path = os.path.join(savedir, f\"{dataName}_landmarks_coordinates.csv\")\n",
    "    roi_df = pd.read_csv(roi_coordinates_path)\n",
    "    \n",
    "    # ★★★ 動画ごとにリストとDataFrameを初期化 ★★★\n",
    "    all_segments_data = []\n",
    "    all_segments_df = pd.DataFrame()\n",
    "    \n",
    "    # ROIごとにRGB、HSVを記録\n",
    "    for roi_idx, row in roi_df.iterrows():\n",
    "        landmark_id = int(row['landmark_id'])  # 整数に変換\n",
    "        center_x = int(row['x'])  # 整数に変換\n",
    "        center_y = int(row['y'])  # 整数に変換\n",
    "        \n",
    "        # パッチの座標を計算（中心座標から±PATCH_SIZE/2）\n",
    "        half_patch = PATCH_SIZE // 2\n",
    "        landmark_left = int(max(0, center_x - half_patch))\n",
    "        landmark_top = int(max(0, center_y - half_patch))\n",
    "        landmark_right = int(center_x + half_patch)\n",
    "        landmark_bottom = int(center_y + half_patch)\n",
    "        \n",
    "        roi = (landmark_left, landmark_top, landmark_right, landmark_bottom)\n",
    "        \n",
    "        print(f'  Processing landmark_id: {landmark_id} at ({center_x}, {center_y})')\n",
    "        \n",
    "        landmark_df = make_analysis_df(\n",
    "            video_path=inputMoviePath,\n",
    "            roi_coordinates=roi,\n",
    "            fps=fps\n",
    "        )\n",
    "        landmark_df[\"landmark_id\"] = landmark_id\n",
    "        print(f'  landmark {landmark_id} data shape: {landmark_df.shape}')\n",
    "        \n",
    "        # 動画のデータをリストに追加\n",
    "        all_segments_data.append(landmark_df)\n",
    "    \n",
    "    # 全てのランドマークのデータを統合\n",
    "    all_landmarks_df = pd.concat(all_segments_data, ignore_index=True)\n",
    "    print(f'Total data shape for {dataName}: {all_landmarks_df.shape}')\n",
    "    \n",
    "    # CSVとして保存\n",
    "    output_csv_path = os.path.join(rootDir, SAVE_DIR, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_landmarks_data.csv\")\n",
    "    all_landmarks_df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Saved: {output_csv_path}')\n",
    "    \n",
    "    # === ROI可視化 ===\n",
    "    # 1フレーム目を読み込み\n",
    "    cap = cv2.VideoCapture(inputMoviePath)\n",
    "    ret, first_frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        vis_frame = first_frame.copy()\n",
    "        \n",
    "        # CSVから各ランドマークのROIを描画\n",
    "        for roi_idx, row in roi_df.iterrows():\n",
    "            landmark_id = int(row['landmark_id'])\n",
    "            center_x = int(row['x'])\n",
    "            center_y = int(row['y'])\n",
    "            \n",
    "            # パッチの座標を計算\n",
    "            half_patch = PATCH_SIZE // 2\n",
    "            left = int(max(0, center_x - half_patch))\n",
    "            top = int(max(0, center_y - half_patch))\n",
    "            right = int(center_x + half_patch)\n",
    "            bottom = int(center_y + half_patch)\n",
    "            \n",
    "            # ROIの矩形を描画（緑色）\n",
    "            cv2.rectangle(vis_frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            \n",
    "            # ランドマークIDを表示\n",
    "            cv2.putText(vis_frame, str(landmark_id), (center_x - 10, center_y - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "            \n",
    "            # 中心点を描画（赤色）\n",
    "            cv2.circle(vis_frame, (center_x, center_y), 3, (0, 0, 255), -1)\n",
    "        \n",
    "        # 可視化画像を保存\n",
    "        vis_output_path = os.path.join(savedir, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_ROI_visualization.png\")\n",
    "        cv2.imwrite(vis_output_path, vis_frame)\n",
    "        print(f'ROI可視化画像を保存: {vis_output_path}\\n')\n",
    "    else:\n",
    "        print(f'Error: フレームの読み込みに失敗しました\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe90449",
   "metadata": {},
   "source": [
    "各窓にRGB信号を格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933183b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZES = [2, 4, 6, 8, 10]\n",
    "STRIDES = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_full_string(arr):\n",
    "    \"\"\"NumPy配列を省略なしの文字列に変換\"\"\"\n",
    "    if isinstance(arr, str):\n",
    "        return arr\n",
    "    elif isinstance(arr, (np.ndarray, list)):\n",
    "        # NumPy配列またはリストを完全な文字列に変換\n",
    "        arr_np = np.array(arr)\n",
    "        return np.array2string(arr_np, threshold=np.inf, max_line_width=np.inf, separator=' ')\n",
    "    else:\n",
    "        return str(arr)\n",
    "\n",
    "class StrideSegmentCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_sizes: List[float] = [2, 3, 4, 5],\n",
    "        strides: List[float] = [0.1, 0.5, 1, 1.5, 2],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_sizes : List[float]\n",
    "            窓幅（秒）のリスト\n",
    "        strides : List[float]\n",
    "            移動秒数のリスト\n",
    "        \"\"\"\n",
    "        self.window_sizes = window_sizes\n",
    "        self.strides = strides\n",
    "\n",
    "    def calculate_overlap(self, window_size: float, stride: float) -> float:\n",
    "        \"\"\"\n",
    "        窓幅と移動秒数からオーバーラップ率を計算\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_size : float\n",
    "            窓幅（秒）\n",
    "        stride : float\n",
    "            移動秒数（秒）\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            オーバーラップ率（%）\n",
    "        \"\"\"\n",
    "        if stride >= window_size:\n",
    "            return 0\n",
    "        overlap = (window_size - stride) / window_size * 100\n",
    "        return round(overlap, 2)\n",
    "\n",
    "    def calculate_segments(\n",
    "        self, window_size: float, stride: float, total_frames: int, fps: int\n",
    "    ) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        フレーム数から解析区間を計算\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_size : float\n",
    "            窓幅（秒）\n",
    "        stride : float\n",
    "            移動秒数（秒）\n",
    "        total_frames : int\n",
    "            総フレーム数\n",
    "        fps : int\n",
    "            フレームレート\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        List[Tuple[int, int]]\n",
    "            各区間の(開始フレーム, 終了フレーム)のリスト\n",
    "        \"\"\"\n",
    "        frames_per_window = round(window_size * fps)\n",
    "        frames_per_stride = round(stride * fps)\n",
    "\n",
    "        segments = []\n",
    "        start_frame = 0\n",
    "\n",
    "        while start_frame + frames_per_window <= total_frames:\n",
    "            segments.append((start_frame, start_frame + frames_per_window))\n",
    "            start_frame += frames_per_stride\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def create_analysis_dataframe(self, total_frames: int, fps: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        全ての窓幅と移動秒数の組み合わせに対してDataFrameを生成\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        total_frames : int\n",
    "            総フレーム数\n",
    "        fps : int\n",
    "            フレームレート\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            各条件でのセグメント情報を含むDataFrame\n",
    "            columns: window_size, stride, overlap, segment_number, frame_start, frame_end\n",
    "        \"\"\"\n",
    "        data_dict = {\n",
    "            \"window_size\": [],\n",
    "            \"stride\": [],\n",
    "            \"overlap\": [],\n",
    "            \"segment_number\": [],\n",
    "            \"frame_start\": [],\n",
    "            \"frame_end\": [],\n",
    "        }\n",
    "\n",
    "        for window_size in self.window_sizes:\n",
    "            for stride in self.strides:\n",
    "                overlap = self.calculate_overlap(window_size, stride)\n",
    "                segments = self.calculate_segments(\n",
    "                    window_size, stride, total_frames, fps\n",
    "                )\n",
    "\n",
    "                for i, (start_frame, end_frame) in enumerate(segments):\n",
    "                    data_dict[\"window_size\"].append(window_size)\n",
    "                    data_dict[\"stride\"].append(stride)\n",
    "                    data_dict[\"overlap\"].append(overlap)\n",
    "                    data_dict[\"segment_number\"].append(i)\n",
    "                    data_dict[\"frame_start\"].append(start_frame)\n",
    "                    data_dict[\"frame_end\"].append(end_frame)\n",
    "\n",
    "        return pd.DataFrame(data_dict)\n",
    "\n",
    "\n",
    "class PulseAnalysisDataStrides:\n",
    "    def __init__(self, window_sizes, strides):\n",
    "        # 窓枠とストライドの値を定義\n",
    "        self.window_sizes = window_sizes\n",
    "        self.strides = strides\n",
    "\n",
    "        # accuracyのみを格納するDataFrameを初期化\n",
    "        self.results = pd.DataFrame(\n",
    "            index=pd.Index(self.window_sizes, name=\"window_size\"),\n",
    "            columns=pd.Index(self.strides, name=\"strides\"),\n",
    "        )\n",
    "\n",
    "    def add_accuracy(self, window_size: float, strides: int, accuracy: float):\n",
    "        \"\"\"\n",
    "        精度データを追加する\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_size : float\n",
    "            窓幅（秒）\n",
    "        strides : int\n",
    "            ストライド（s）\n",
    "        accuracy : float\n",
    "            精度値\n",
    "        \"\"\"\n",
    "        self.results.loc[window_size, strides] = accuracy\n",
    "\n",
    "    def _create_heatmap_dataframe(self):\n",
    "        \"\"\"\n",
    "        ヒートマップ用のDataFrameを作成する内部メソッド\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            ヒートマップ用に整形されたDataFrame\n",
    "        \"\"\"\n",
    "        data = {\"stride\": self.strides}\n",
    "        for window_size in self.window_sizes:\n",
    "            data[window_size] = [\n",
    "                self.results.loc[window_size, stride] for stride in self.strides\n",
    "            ]\n",
    "        df = pd.DataFrame(data).set_index(\"stride\").T\n",
    "        return df\n",
    "\n",
    "    def save_heatmap(\n",
    "        self,\n",
    "        title: str,\n",
    "        save_path: str,\n",
    "        figsize: tuple = (10, 8),\n",
    "        cmap: str = \"YlGnBu\",\n",
    "        colorbar_label: str = \"MAE\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ヒートマップを作成して保存する\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        title : str\n",
    "            プロットのタイトル\n",
    "        save_path : str\n",
    "            保存先のパス\n",
    "        figsize : tuple, optional\n",
    "            図のサイズ (default: (10, 8))\n",
    "        cmap : str, optional\n",
    "            カラーマップ (default: 'YlGnBu')\n",
    "        colorbar_label : str, optional\n",
    "            カラーバーのラベル (default: 'MAE')\n",
    "        \"\"\"\n",
    "        df = self._create_heatmap_dataframe()\n",
    "\n",
    "        # ヒートマップを作成\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(\n",
    "            df, annot=True, fmt=\".4f\", cmap=cmap, cbar_kws={\"label\": colorbar_label}\n",
    "        )\n",
    "        plt.title(f\"{title}\")\n",
    "        plt.xlabel(\"Stride [s]\")\n",
    "        plt.ylabel(\"Window Size [s]\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def save_heatmap_std(self, title: str, save_path: str, figsize: tuple = (10, 8)):\n",
    "        \"\"\"\n",
    "        標準偏差のヒートマップを作成して保存する\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        title : str\n",
    "            プロットのタイトル\n",
    "        save_path : str\n",
    "            保存先のパス\n",
    "        figsize : tuple, optional\n",
    "            図のサイズ (default: (10, 8))\n",
    "        \"\"\"\n",
    "        self.save_heatmap(\n",
    "            title,\n",
    "            save_path,\n",
    "            figsize,\n",
    "            cmap=\"Reds\",\n",
    "            colorbar_label=\"Standard Deviation\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(movie_paths)):\n",
    "    inputMoviePath = movie_paths[i]\n",
    "    rootDir = data_dirs[i]\n",
    "    dataName = movie_names[i]\n",
    "\n",
    "    print(f'Processing movie: {inputMoviePath}')\n",
    "\n",
    "    # 動画のfpsを取得\n",
    "    cap = cv2.VideoCapture(inputMoviePath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    samplingRate = fps\n",
    "    cap.release()\n",
    "\n",
    "    save_dir = os.path.join(rootDir, SAVE_DIR)\n",
    "    os.makedirs(save_dir, exist_ok=True)        \n",
    "    \n",
    "\n",
    "    # CSVファイルの読み込み\n",
    "    ecg_csv_path = os.path.join(rootDir, dataName + '.csv')\n",
    "    ecg_df = pd.read_csv(ecg_csv_path)\n",
    "\n",
    "    ecg_RRI_csv_path = os.path.join(rootDir, f'RRI_Simple_{movie_names[i]}.csv')\n",
    "    ecg_RRI_df = pd.read_csv(ecg_RRI_csv_path)\n",
    "\n",
    "    # landmarkで共通のecg_bpm_in_window_meanを取得\n",
    "    ecg_bpm_in_window = ecg_RRI_df['BPM']\n",
    "    ecg_bpm_in_window_mean = ecg_RRI_df['BPM'].values.mean()\n",
    "\n",
    "    # ROIデータの読み込み\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    landmark_data_path = os.path.join(rootDir, SAVE_DIR, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_landmarks_data.csv\")\n",
    "    landmark_data_df = pd.read_csv(landmark_data_path)\n",
    "\n",
    "    stride_segment_calculator = StrideSegmentCalculator(window_sizes=WINDOW_SIZES, strides=STRIDES)\n",
    "    analysis_df = stride_segment_calculator.create_analysis_dataframe(total_frames, fps)\n",
    "\n",
    "    # ランドマークごとに実行\n",
    "    # 結果を格納するリスト\n",
    "    all_window_results = []\n",
    "    for idx, row in analysis_df.iterrows():\n",
    "        window_size = row['window_size']\n",
    "        frame_start = row['frame_start']\n",
    "        frame_end = row['frame_end']\n",
    "        stride = row['stride']\n",
    "\n",
    "        # 窓の時間範囲を計算\n",
    "        window_start_time = frame_start / fps\n",
    "        window_end_time = frame_end / fps\n",
    "\n",
    "        if(window_end_time > total_frames / fps):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nウィンドウ {idx}: 窓サイズ {window_size}s, ストライド {stride}s, フレーム {frame_start}-{frame_end}, 時間 {window_start_time:.2f}-{window_end_time:.2f}s\")\n",
    "\n",
    "        # 該当する窓の時間範囲内の真値RRIデータを抽出\n",
    "        ecg_RRI_mask = (ecg_RRI_df['time'] >= window_start_time) & \\\n",
    "                (ecg_RRI_df['time'] < window_end_time)\n",
    "        ecg_bpm_in_window = ecg_RRI_df[ecg_RRI_mask]['BPM'].values\n",
    "        ecg_bpm_in_window_mean = np.mean(ecg_bpm_in_window) if len(ecg_bpm_in_window) > 0 else np.nan\n",
    "        print(f'    True Value RRI count in window: {len(ecg_bpm_in_window)}, Mean BPM: {ecg_bpm_in_window_mean:.2f}')\n",
    "        \n",
    "        for landmark_id in landmark_data_df['landmark_id'].unique():\n",
    "            print(f'  Analyzing landmark_id: {landmark_id}')\n",
    "            landmark_df = landmark_data_df[landmark_data_df['landmark_id'] == landmark_id]\n",
    "            print(f'    landmark data shape: {landmark_df}')\n",
    "            \n",
    "\n",
    "            # 該当する窓の時間範囲内のRGB信号を抽出し、窓内でBVP算出\n",
    "            bvp_mask = (landmark_df['timestamp'] >= window_start_time) & (landmark_df['timestamp'] < window_end_time)\n",
    "\n",
    "            r_signal_in_window = landmark_df[bvp_mask]['r_mean'].values\n",
    "            g_signal_in_window = landmark_df[bvp_mask]['g_mean'].values\n",
    "            b_signal_in_window = landmark_df[bvp_mask]['b_mean'].values\n",
    "\n",
    "            saturation_signal_in_window = landmark_df[bvp_mask]['s_mean'].values\n",
    "            lightness_signal_in_window = landmark_df[bvp_mask]['l_mean'].values\n",
    "\n",
    "            # 窓情報を保存\n",
    "            window_info = {\n",
    "                'landmark_id': landmark_id,\n",
    "                'window_index': idx,\n",
    "                'window_size': window_size,\n",
    "                'stride': stride,\n",
    "                'frame_start': frame_start,\n",
    "                'frame_end': frame_end,\n",
    "                'window_start_time': window_start_time,\n",
    "                'window_end_time': window_end_time,\n",
    "                'r_signal_in_window': array_to_full_string(r_signal_in_window),\n",
    "                'g_signal_in_window': array_to_full_string(g_signal_in_window),\n",
    "                'b_signal_in_window': array_to_full_string(b_signal_in_window),\n",
    "                'saturation_signal_in_window': array_to_full_string(saturation_signal_in_window),\n",
    "                'lightness_signal_in_window': array_to_full_string(lightness_signal_in_window)\n",
    "            }\n",
    "            all_window_results.append(window_info)\n",
    "    # 全結果をDataFrameに変換して保存\n",
    "    results_df = pd.DataFrame(all_window_results)\n",
    "    results_save_dir = os.path.join(rootDir, SAVE_DIR)\n",
    "    os.makedirs(results_save_dir, exist_ok=True)\n",
    "    results_csv_path = os.path.join(results_save_dir, f'{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_ROI_window_signals.csv')\n",
    "    results_df.to_csv(results_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nFFT結果をCSVに保存: {results_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8614501",
   "metadata": {},
   "source": [
    "### 窓ごとにBVPとMAEを算出し、保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_BVPsignal(r_signals, g_signals, b_signals,  fps, deviceType, bvpMethod, bvpMethodName, method_params=None):\n",
    "    \"\"\"\n",
    "    RGB信号からBVP信号を抽出する関数\n",
    "    \"\"\"\n",
    "    rgb_signal = np.array([[r_signals, g_signals, b_signals]], dtype=np.float32)\n",
    "    print(f\"\\nRGB信号の形状: {rgb_signal.shape}\")\n",
    "    \n",
    "    signal_length = rgb_signal.shape[2]\n",
    "    min_required_length = 50\n",
    "    \n",
    "    if signal_length < min_required_length:\n",
    "        print(f\"警告: 信号長が短すぎます ({signal_length} < {min_required_length})。処理をスキップします。\")\n",
    "        return None, None\n",
    "    \n",
    "    filtered_signal = [rgb_signal]\n",
    "    \n",
    "    # デフォルトのメソッドパラメータ\n",
    "    if method_params is None:\n",
    "        method_params = {}\n",
    "    \n",
    "    # メソッド別パラメータ設定\n",
    "    if bvpMethodName in [\"cupy_POS\", \"cpu_POS\"]:\n",
    "        method_params['fps'] = fps\n",
    "    elif bvpMethodName in [\"cpu_ICA\", \"cpu_PCA\"]:\n",
    "        method_params['component'] = 'all_comp'\n",
    "    \n",
    "    print(f\"\\nBVP抽出開始 (メソッド: {bvpMethodName})\")\n",
    "    print(f\"パラメータ: {method_params}\")\n",
    "    \n",
    "    # BVP信号抽出\n",
    "    if method_params:\n",
    "        bvp_signal = vhr.BVP.RGB_sig_to_BVP(\n",
    "            filtered_signal,\n",
    "            fps,\n",
    "            device_type=deviceType,\n",
    "            method=bvpMethod,\n",
    "            params=method_params\n",
    "        )\n",
    "    else:\n",
    "        bvp_signal = vhr.BVP.RGB_sig_to_BVP(\n",
    "            filtered_signal,\n",
    "            fps,\n",
    "            device_type=deviceType,\n",
    "            method=bvpMethod\n",
    "        )\n",
    "    \n",
    "    # 生のBVP信号を保存\n",
    "    raw_bvp_signal = bvp_signal[0].copy() if len(bvp_signal) > 0 else None\n",
    "    \n",
    "    # 後処理フィルタリング\n",
    "    bvp_signal = vhr.BVP.apply_filter(\n",
    "        bvp_signal,\n",
    "        vhr.BVP.BPfilter,\n",
    "        params={'order': 6, 'minHz': 0.5, 'maxHz': 2.0, 'fps': fps}\n",
    "    )\n",
    "    \n",
    "    bvp_signal = vhr.BVP.apply_filter(bvp_signal, vhr.BVP.zeromean)\n",
    "    \n",
    "    filtered_bvp_signal = bvp_signal[0] if len(bvp_signal) > 0 else None\n",
    "    \n",
    "    print(f\"\\nBVP信号抽出完了\")\n",
    "    return raw_bvp_signal, filtered_bvp_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_window_fft(values, fps):\n",
    "    \"\"\"\n",
    "    時系列データの最大周波数とスペクトル情報を計算する関数\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : array-like\n",
    "        分析対象の時系列データ\n",
    "    fps : int\n",
    "        サンプリング周波数（1秒あたりのフレーム数）\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        以下のキーを含む辞書：\n",
    "        - 'max_freq': 検出された最大周波数\n",
    "        - 'max_amplitude': 最大周波数のときの振幅\n",
    "        - 'frequencies': 周波数配列（正の周波数のみ）\n",
    "        - 'amplitudes': 振幅配列（正の周波数のみ）\n",
    "        - 'power_spectrum': パワースペクトル\n",
    "        - 'dominant_freqs': 上位5つの卓越周波数とその振幅\n",
    "        - 'spectral_centroid': スペクトル重心\n",
    "        - 'spectral_bandwidth': スペクトル帯域幅\n",
    "        - 'total_power': 全体のパワー\n",
    "    \"\"\"\n",
    "    # データをnumpy配列に変換\n",
    "    values = np.array(values)\n",
    "\n",
    "    # ゼロパディングで分解能を向上（窓長の8倍）\n",
    "    n_pad = len(values) * 8\n",
    "\n",
    "    # ハミング窓を適用（オプション：コメントアウトされている）\n",
    "    # window = np.hamming(len(values))\n",
    "    # windowed_data = values * window\n",
    "\n",
    "    # FFTを実行（ゼロパディング適用）\n",
    "    fft_result = np.fft.fft(values, n=n_pad)\n",
    "    fft_freq = np.fft.fftfreq(n_pad, 1 / fps)\n",
    "\n",
    "    # 正の周波数のみを取得\n",
    "    positive_freq_idx = fft_freq > 0\n",
    "    positive_fft = np.abs(fft_result[positive_freq_idx])\n",
    "    positive_freq = fft_freq[positive_freq_idx]\n",
    "    \n",
    "    # パワースペクトルを計算\n",
    "    power_spectrum = positive_fft ** 2\n",
    "\n",
    "    # 最大周波数の検出と補間\n",
    "    max_idx = np.argmax(positive_fft)\n",
    "    max_amplitude = positive_fft[max_idx]\n",
    "    \n",
    "    if 0 < max_idx < len(positive_fft) - 1:\n",
    "        # 3点を使用した放物線補間\n",
    "        alpha = positive_fft[max_idx - 1]\n",
    "        beta = positive_fft[max_idx]\n",
    "        gamma = positive_fft[max_idx + 1]\n",
    "        peak_pos = 0.5 * (alpha - gamma) / (alpha - 2 * beta + gamma)\n",
    "\n",
    "        # 補間された周波数と振幅\n",
    "        freq_resolution = fps / n_pad\n",
    "        max_freq = positive_freq[max_idx] + peak_pos * freq_resolution\n",
    "        \n",
    "        # 補間された振幅（放物線の頂点）\n",
    "        max_amplitude = beta - 0.25 * (alpha - gamma) * peak_pos\n",
    "    else:\n",
    "        max_freq = positive_freq[max_idx]\n",
    "    \n",
    "    # 上位5つの卓越周波数を検出\n",
    "    top_indices = np.argsort(positive_fft)[-5:][::-1]\n",
    "    dominant_freqs = [(positive_freq[idx], positive_fft[idx]) for idx in top_indices]\n",
    "    \n",
    "    # スペクトル特徴量の計算\n",
    "    # スペクトル重心（周波数の重み付き平均）\n",
    "    spectral_centroid = np.sum(positive_freq * positive_fft) / np.sum(positive_fft)\n",
    "    \n",
    "    # スペクトル帯域幅（重心からの重み付き分散）\n",
    "    spectral_bandwidth = np.sqrt(\n",
    "        np.sum(((positive_freq - spectral_centroid) ** 2) * positive_fft) / np.sum(positive_fft)\n",
    "    )\n",
    "    \n",
    "    # 全体のパワー\n",
    "    total_power = np.sum(power_spectrum)\n",
    "    \n",
    "    # 結果を辞書にまとめる\n",
    "    result = {\n",
    "        'max_freq': max_freq,\n",
    "        'max_amplitude': max_amplitude,\n",
    "        'frequencies': positive_freq,\n",
    "        'amplitudes': positive_fft,\n",
    "        'power_spectrum': power_spectrum,\n",
    "        'dominant_freqs': dominant_freqs,\n",
    "        'spectral_centroid': spectral_centroid,\n",
    "        'spectral_bandwidth': spectral_bandwidth,\n",
    "        'total_power': total_power,\n",
    "        'freq_resolution': fps / n_pad,  # 周波数分解能\n",
    "        'nyquist_freq': fps / 2  # ナイキスト周波数\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e81a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(movie_paths)):\n",
    "    inputMoviePath = movie_paths[i]\n",
    "    rootDir = data_dirs[i]\n",
    "    dataName = movie_names[i]\n",
    "\n",
    "    print(f'Processing movie: {inputMoviePath}')\n",
    "\n",
    "    # 動画のfpsを取得\n",
    "    cap = cv2.VideoCapture(inputMoviePath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    samplingRate = fps\n",
    "    cap.release()\n",
    "\n",
    "    save_dir = os.path.join(rootDir, SAVE_DIR)\n",
    "    os.makedirs(save_dir, exist_ok=True)        \n",
    "\n",
    "    # CSVファイルの読み込み\n",
    "    ecg_csv_path = os.path.join(rootDir, dataName + '.csv')\n",
    "    ecg_df = pd.read_csv(ecg_csv_path)\n",
    "\n",
    "    ecg_RRI_csv_path = os.path.join(rootDir, f'RRI_Simple_{movie_names[i]}.csv')\n",
    "    ecg_RRI_df = pd.read_csv(ecg_RRI_csv_path)\n",
    "\n",
    "    # landmarkで共通のecg_bpm_in_window_meanを取得\n",
    "    ecg_bpm_in_window = ecg_RRI_df['BPM']\n",
    "    ecg_bpm_in_window_mean = ecg_RRI_df['BPM'].values.mean()\n",
    "\n",
    "    # ROIデータの読み込み\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    window_signals_data_path = os.path.join(rootDir, SAVE_DIR, f'{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_ROI_window_signals.csv')\n",
    "    window_signals_data_df = pd.read_csv(window_signals_data_path)\n",
    "\n",
    "    stride_segment_calculator = StrideSegmentCalculator(window_sizes=WINDOW_SIZES, strides=STRIDES)\n",
    "    analysis_df = stride_segment_calculator.create_analysis_dataframe(total_frames, fps)\n",
    "\n",
    "    # ランドマークごとに実行\n",
    "    # 結果を格納するリスト\n",
    "    all_window_results = []\n",
    "    for idx, row in analysis_df.iterrows():\n",
    "        window_size = row['window_size']\n",
    "        frame_start = row['frame_start']\n",
    "        frame_end = row['frame_end']\n",
    "        stride = row['stride']\n",
    "\n",
    "        # 窓の時間範囲を計算\n",
    "        window_start_time = frame_start / fps\n",
    "        window_end_time = frame_end / fps\n",
    "\n",
    "        if(window_end_time > total_frames / fps):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nウィンドウ {idx}: 窓サイズ {window_size}s, ストライド {stride}s, フレーム {frame_start}-{frame_end}, 時間 {window_start_time:.2f}-{window_end_time:.2f}s\")\n",
    "\n",
    "        # 該当する窓の時間範囲内の真値RRIデータを抽出\n",
    "        ecg_RRI_mask = (ecg_RRI_df['time'] >= window_start_time) & \\\n",
    "                (ecg_RRI_df['time'] < window_end_time)\n",
    "        ecg_bpm_in_window = ecg_RRI_df[ecg_RRI_mask]['BPM'].values\n",
    "        ecg_bpm_in_window_mean = np.mean(ecg_bpm_in_window) if len(ecg_bpm_in_window) > 0 else np.nan\n",
    "        print(f'    True Value RRI count in window: {len(ecg_bpm_in_window)}, Mean BPM: {ecg_bpm_in_window_mean:.2f}')\n",
    "        \n",
    "        for landmark_id in window_signals_data_df['landmark_id'].unique():\n",
    "            print(f'  Analyzing landmark_id: {landmark_id}')\n",
    "            landmark_df = window_signals_data_df[window_signals_data_df['landmark_id'] == landmark_id]\n",
    "\n",
    "            # 該当する窓の時間範囲内のRGB信号を抽出し、窓内でBVP算出\n",
    "            bvp_mask = landmark_df['window_index'] == idx\n",
    "\n",
    "            r_signal_in_window = landmark_df[bvp_mask]['r_signal_in_window'].values\n",
    "            g_signal_in_window = landmark_df[bvp_mask]['g_signal_in_window'].values\n",
    "            b_signal_in_window = landmark_df[bvp_mask]['b_signal_in_window'].values\n",
    "            saturation_signal_in_window = landmark_df[bvp_mask]['saturation_signal_in_window'].values\n",
    "            lightness_signal_in_window = landmark_df[bvp_mask]['lightness_signal_in_window'].values\n",
    "\n",
    "            # 文字列をNumPy配列に変換\n",
    "            r_signal_in_window = np.fromstring(r_signal_in_window[0].strip('[]'), sep=' ') if len(r_signal_in_window) > 0 else np.array([])\n",
    "            g_signal_in_window = np.fromstring(g_signal_in_window[0].strip('[]'), sep=' ') if len(g_signal_in_window) > 0 else np.array([])\n",
    "            b_signal_in_window = np.fromstring(b_signal_in_window[0].strip('[]'), sep=' ') if len(b_signal_in_window) > 0 else np.array([])\n",
    "            saturation_signal_in_window = np.fromstring(saturation_signal_in_window[0].strip('[]'), sep=' ') if len(saturation_signal_in_window) > 0 else np.array([])\n",
    "            lightness_signal_in_window = np.fromstring(lightness_signal_in_window[0].strip('[]'), sep=' ') if len(lightness_signal_in_window) > 0 else np.array([])\n",
    "\n",
    "            print(f'    RGB信号の長さ: R={len(r_signal_in_window)}, G={len(g_signal_in_window)}, B={len(b_signal_in_window)}')\n",
    "\n",
    "            # BVPメソッドの設定\n",
    "            methodCombinations =  ['cuda', cupy_POS, \"cupy_POS\"]\n",
    "            deviceType = methodCombinations[0]  # 'cuda'\n",
    "            bvpMethod = methodCombinations[1]   # cupy_POS\n",
    "            bvpMethodName = methodCombinations[2]  # \"cupy_POS\"\n",
    "\n",
    "            # rgbからBVPを計算\n",
    "            raw_bvp_signal_in_window, filtered_bvp_signal_in_window = extract_BVPsignal(\n",
    "                r_signal_in_window,\n",
    "                g_signal_in_window,\n",
    "                b_signal_in_window,\n",
    "                fps,\n",
    "                deviceType,\n",
    "                bvpMethod,\n",
    "                bvpMethodName\n",
    "            )\n",
    "            \n",
    "            # BVP信号の抽出に失敗した場合はスキップ\n",
    "            if filtered_bvp_signal_in_window is None:\n",
    "                print(f\"ウィンドウ {idx} をスキップ: BVP信号の抽出に失敗\")\n",
    "                continue\n",
    "            \n",
    "            # FFT解析\n",
    "            raw_bvp_signal_in_window = raw_bvp_signal_in_window.flatten() if raw_bvp_signal_in_window is not None else None\n",
    "            filtered_bvp_signal_in_window = filtered_bvp_signal_in_window.flatten()\n",
    "            fft_result_dic = analyze_window_fft(filtered_bvp_signal_in_window, fps)\n",
    "\n",
    "            # MAEの計算\n",
    "            rppg_bpm = fft_result_dic['max_freq'] * 60\n",
    "            rppg_freq = fft_result_dic['frequencies']\n",
    "            rppg_amplitude = fft_result_dic['amplitudes']\n",
    "            rppg_pwd = fft_result_dic['power_spectrum']\n",
    "\n",
    "            bpm_MAE = np.abs(ecg_bpm_in_window_mean - rppg_bpm) if not np.isnan(ecg_bpm_in_window_mean) else np.nan\n",
    "\n",
    "            # 窓情報を保存\n",
    "            window_info = {\n",
    "                'landmark_id': landmark_id,\n",
    "                'window_index': idx,\n",
    "                'window_size': window_size,\n",
    "                'stride': stride,\n",
    "                'frame_start': frame_start,\n",
    "                'frame_end': frame_end,\n",
    "                'window_start_time': window_start_time,\n",
    "                'window_end_time': window_end_time,\n",
    "                'r_signal_in_window': array_to_full_string(r_signal_in_window),\n",
    "                'g_signal_in_window': array_to_full_string(g_signal_in_window),\n",
    "                'b_signal_in_window': array_to_full_string(b_signal_in_window),\n",
    "                'saturation_signal_in_window': array_to_full_string(saturation_signal_in_window),\n",
    "                'lightness_signal_in_window': array_to_full_string(lightness_signal_in_window),\n",
    "                'raw_bvp_in_window': raw_bvp_signal_in_window,\n",
    "                'filtered_bvp_in_window': filtered_bvp_signal_in_window,\n",
    "                'ecg_bpm_in_window': ecg_bpm_in_window,\n",
    "                'ecg_bpm_mean': ecg_bpm_in_window_mean,\n",
    "                'rppg_bpm': rppg_bpm,\n",
    "                'bpm_MAE': bpm_MAE,\n",
    "                'max_freq': fft_result_dic['max_freq'],\n",
    "                'max_amplitude': fft_result_dic['max_amplitude'],\n",
    "                'spectral_centroid': fft_result_dic['spectral_centroid'],\n",
    "                'spectral_bandwidth': fft_result_dic['spectral_bandwidth'],\n",
    "                'total_power': fft_result_dic['total_power']\n",
    "            }\n",
    "            all_window_results.append(window_info)\n",
    "    # 全結果をDataFrameに変換して保存\n",
    "    results_df = pd.DataFrame(all_window_results)\n",
    "    results_save_dir = os.path.join(rootDir, SAVE_DIR)\n",
    "    os.makedirs(results_save_dir, exist_ok=True)\n",
    "    results_csv_path = os.path.join(results_save_dir, f'{dataName}_window_analysis_{PATCH_SIZE}x{PATCH_SIZE}.csv')\n",
    "    results_df.to_csv(results_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nFFT結果をCSVに保存: {results_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c49da",
   "metadata": {},
   "source": [
    "### 記録した座標を読み込み、指標を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83406faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skin_angle(r_signal, g_signal, b_signal):\n",
    "    \"\"\"\n",
    "    皮膚色角度を計算（標準肌色調ベクトルとの角度）\n",
    "    \n",
    "    Args:\n",
    "        r_signal: (N,) 赤チャンネル信号\n",
    "        g_signal: (N,) 緑チャンネル信号\n",
    "        b_signal: (N,) 青チャンネル信号\n",
    "    \n",
    "    Returns:\n",
    "        angle: float 標準肌色調ベクトル [0.7682, 0.5121, 0.3841] との角度（度）\n",
    "    \"\"\"\n",
    "    eps = 1e-9  # ゼロ除算対策\n",
    "    \n",
    "    # RGB統計量（時間平均）\n",
    "    r_mean = np.mean(r_signal)\n",
    "    g_mean = np.mean(g_signal)\n",
    "    b_mean = np.mean(b_signal)\n",
    "    \n",
    "    # RGB正規化（単位ベクトル化）\n",
    "    norm = np.sqrt(r_mean**2 + g_mean**2 + b_mean**2)\n",
    "    \n",
    "    # ゼロ除算チェック\n",
    "    if norm < eps:\n",
    "        print(\"警告: RGB平均値がほぼゼロです。角度を計算できません。\")\n",
    "        return np.nan\n",
    "    \n",
    "    skin_vector = np.array([r_mean, g_mean, b_mean]) / norm\n",
    "    \n",
    "    # 標準化肌ベクトル [0.7682, 0.5121, 0.3841]との角度計算\n",
    "    reference_vector = np.array([0.7682, 0.5121, 0.3841])\n",
    "    \n",
    "    # コサイン類似度（範囲を[-1, 1]にクリップ）\n",
    "    cosine_similarity = np.dot(skin_vector, reference_vector)\n",
    "    cosine_similarity = np.clip(cosine_similarity, -1.0, 1.0)\n",
    "    \n",
    "    # 角度計算（度数法）\n",
    "    angle = np.degrees(np.arccos(cosine_similarity))\n",
    "    \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d99b8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing movie: 202511071715\n",
      "  Processing landmark_id: 119 at (325, 451)\n",
      "  landmark 119 data shape: (5400, 7)\n",
      "  Processing landmark_id: 207 at (304, 512)\n",
      "  landmark 207 data shape: (5400, 7)\n",
      "  Processing landmark_id: 347 at (466, 453)\n",
      "  landmark 347 data shape: (5400, 7)\n",
      "  Processing landmark_id: 425 at (456, 493)\n",
      "  landmark 425 data shape: (5400, 7)\n",
      "  Processing landmark_id: 4 at (384, 476)\n",
      "  landmark 4 data shape: (5400, 7)\n",
      "  Processing landmark_id: 200 at (386, 588)\n",
      "  landmark 200 data shape: (5400, 7)\n",
      "Total data shape for 202511071715: (32400, 7)\n",
      "Saved: experimentData\\202511071715\\LandmarksAccuracyEvalResults\\202511071715_30.0x30.0_landmarks_angles_data.csv\n"
     ]
    }
   ],
   "source": [
    "for movie_idx in range(len(movie_paths)):  \n",
    "    inputMoviePath = movie_paths[movie_idx]\n",
    "    rootDir = data_dirs[movie_idx]\n",
    "    dataName = movie_names[movie_idx]\n",
    "    \n",
    "    print(f'Processing movie: {dataName}')  \n",
    "    \n",
    "    cap = cv2.VideoCapture(inputMoviePath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    savedir = os.path.join(rootDir, SAVE_DIR)\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    \n",
    "    # ROIの座標を取得\n",
    "    roi_coordinates_path = os.path.join(savedir, f\"{dataName}_landmarks_coordinates.csv\")\n",
    "    roi_df = pd.read_csv(roi_coordinates_path)\n",
    "    \n",
    "    # ★★★ 動画ごとにリストとDataFrameを初期化 ★★★\n",
    "    all_segments_data = []\n",
    "    all_segments_df = pd.DataFrame()\n",
    "    \n",
    "    # ROIごとにRGB、HSVを記録\n",
    "    for roi_idx, row in roi_df.iterrows():\n",
    "        landmark_id = int(row['landmark_id'])\n",
    "        center_x = int(row['x'])\n",
    "        center_y = int(row['y'])\n",
    "        \n",
    "        # パッチの座標を計算\n",
    "        half_patch = PATCH_SIZE // 2\n",
    "        landmark_left = int(max(0, center_x - half_patch))\n",
    "        landmark_top = int(max(0, center_y - half_patch))\n",
    "        landmark_right = int(center_x + half_patch)\n",
    "        landmark_bottom = int(center_y + half_patch)\n",
    "        \n",
    "        roi = (landmark_left, landmark_top, landmark_right, landmark_bottom)\n",
    "        \n",
    "        print(f'  Processing landmark_id: {landmark_id} at ({center_x}, {center_y})')\n",
    "        \n",
    "        # ★★★ リストに辞書を追加していく方法に変更 ★★★\n",
    "        landmark_data_list = []\n",
    "        cap = cv2.VideoCapture(inputMoviePath)\n",
    "        frame_number = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            timestamp = frame_number / fps\n",
    "            \n",
    "            # パッチ領域を抽出\n",
    "            patch = frame[landmark_top:landmark_bottom, landmark_left:landmark_right]\n",
    "            \n",
    "            # RGB平均値を計算\n",
    "            r_mean = np.mean(patch[:, :, 2])\n",
    "            g_mean = np.mean(patch[:, :, 1])\n",
    "            b_mean = np.mean(patch[:, :, 0])\n",
    "\n",
    "            # 皮膚色角度を計算\n",
    "            skin_angle = calculate_skin_angle(\n",
    "                np.array([r_mean]),\n",
    "                np.array([g_mean]),\n",
    "                np.array([b_mean])\n",
    "            )\n",
    "            \n",
    "            # データを辞書としてリストに追加\n",
    "            landmark_data_list.append({\n",
    "                'frame_number': frame_number,\n",
    "                'timestamp': timestamp,\n",
    "                'r_mean': r_mean,\n",
    "                'g_mean': g_mean,\n",
    "                'b_mean': b_mean,\n",
    "                'angle': skin_angle,\n",
    "                'landmark_id': landmark_id\n",
    "            })\n",
    "            \n",
    "            frame_number += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # リストからDataFrameを作成\n",
    "        landmark_df = pd.DataFrame(landmark_data_list)\n",
    "        \n",
    "        print(f'  landmark {landmark_id} data shape: {landmark_df.shape}')\n",
    "        \n",
    "        # 動画のデータをリストに追加\n",
    "        all_segments_data.append(landmark_df)\n",
    "    \n",
    "    # 全てのランドマークのデータを統合\n",
    "    all_landmarks_df = pd.concat(all_segments_data, ignore_index=True)\n",
    "    print(f'Total data shape for {dataName}: {all_landmarks_df.shape}')\n",
    "    \n",
    "    # CSVとして保存\n",
    "    output_csv_path = os.path.join(rootDir, SAVE_DIR, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_landmarks_angles_data.csv\")\n",
    "    all_landmarks_df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Saved: {output_csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d9ba6",
   "metadata": {},
   "source": [
    "### 窓ごとにマスクを作成"
   ]
  },
  {
   "cell_type": "code",
   "id": "gw1gitqhfms",
   "source": "def create_landmark_mask(frame_shape, landmark_x, landmark_y, patch_size):\n    \"\"\"\n    ランドマーク周辺のパッチ領域に対してマスクを作成\n    \n    Parameters:\n    -----------\n    frame_shape : tuple\n        フレームの形状 (height, width)\n    landmark_x : int\n        ランドマークのx座標\n    landmark_y : int\n        ランドマークのy座標\n    patch_size : float\n        パッチサイズ（ピクセル数）\n    \n    Returns:\n    --------\n    mask : np.ndarray\n        バイナリマスク画像（パッチ領域が255、それ以外が0）\n    roi_coords : tuple\n        ROI座標 (left, top, right, bottom)\n    \"\"\"\n    h, w = frame_shape[:2]\n    \n    # パッチの座標を計算\n    half_patch = int(patch_size // 2)\n    left = int(max(0, landmark_x - half_patch))\n    top = int(max(0, landmark_y - half_patch))\n    right = int(min(w, landmark_x + half_patch))\n    bottom = int(min(h, landmark_y + half_patch))\n    \n    # マスク画像を初期化（黒：全て0）\n    mask = np.zeros((h, w), dtype=np.uint8)\n    \n    # パッチ領域を白（255）で塗りつぶし\n    mask[top:bottom, left:right] = 255\n    \n    roi_coords = (left, top, right, bottom)\n    \n    return mask, roi_coords\n\ndef visualize_landmark_masks(frame, landmarks_df, patch_size, landmark_id_name_map):\n    \"\"\"\n    全ランドマークのマスクを可視化\n    \n    Parameters:\n    -----------\n    frame : np.ndarray\n        元の画像フレーム\n    landmarks_df : pd.DataFrame\n        ランドマーク座標を含むDataFrame\n    patch_size : float\n        パッチサイズ\n    landmark_id_name_map : dict\n        ランドマークIDと名前のマッピング\n    \n    Returns:\n    --------\n    vis_frame : np.ndarray\n        マスクを描画した画像\n    \"\"\"\n    vis_frame = frame.copy()\n    h, w = frame.shape[:2]\n    \n    # 全マスクを重ねた画像を作成\n    all_masks = np.zeros((h, w), dtype=np.uint8)\n    \n    for idx, row in landmarks_df.iterrows():\n        landmark_id = int(row['landmark_id'])\n        landmark_x = int(row['x'])\n        landmark_y = int(row['y'])\n        \n        # マスクを作成\n        mask, roi_coords = create_landmark_mask(\n            frame.shape, landmark_x, landmark_y, patch_size\n        )\n        \n        # マスクを累積\n        all_masks = cv2.bitwise_or(all_masks, mask)\n        \n        # ROI矩形を描画（緑色）\n        left, top, right, bottom = roi_coords\n        cv2.rectangle(vis_frame, (left, top), (right, bottom), (0, 255, 0), 2)\n        \n        # ランドマークIDと名前を表示\n        landmark_name = landmark_id_name_map.get(landmark_id, str(landmark_id))\n        label = f\"{landmark_id}: {landmark_name}\"\n        cv2.putText(vis_frame, label, (landmark_x - 10, landmark_y - 10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n        \n        # 中心点を描画（赤色）\n        cv2.circle(vis_frame, (landmark_x, landmark_y), 3, (0, 0, 255), -1)\n    \n    # マスク領域を半透明で重畳表示（緑色）\n    mask_colored = np.zeros_like(vis_frame)\n    mask_colored[:, :, 1] = all_masks  # 緑チャンネル\n    vis_frame = cv2.addWeighted(vis_frame, 0.7, mask_colored, 0.3, 0)\n    \n    return vis_frame",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qfgohhbkw8g",
   "source": "for movie_idx in range(len(movie_paths)):\n    inputMoviePath = movie_paths[movie_idx]\n    rootDir = data_dirs[movie_idx]\n    dataName = movie_names[movie_idx]\n    \n    print(f'\\n{\"=\"*60}')\n    print(f'マスク作成開始: {dataName}')\n    print(f'{\"=\"*60}')\n    \n    cap = cv2.VideoCapture(inputMoviePath)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # 1フレーム目を読み込み\n    ret, first_frame = cap.read()\n    cap.release()\n    \n    if not ret:\n        print(f\"Error: 最初のフレームを読み込めませんでした: {inputMoviePath}\")\n        continue\n    \n    savedir = os.path.join(rootDir, SAVE_DIR)\n    os.makedirs(savedir, exist_ok=True)\n    \n    # ランドマーク座標を読み込み\n    roi_coordinates_path = os.path.join(savedir, f\"{dataName}_landmarks_coordinates.csv\")\n    roi_df = pd.read_csv(roi_coordinates_path)\n    \n    print(f\"ランドマーク数: {len(roi_df)}\")\n    print(f\"パッチサイズ: {PATCH_SIZE}x{PATCH_SIZE} ピクセル\")\n    \n    # === 各ランドマークのマスクを作成して保存 ===\n    masks_dict = {}  # {landmark_id: (mask, roi_coords)}\n    \n    for idx, row in roi_df.iterrows():\n        landmark_id = int(row['landmark_id'])\n        landmark_x = int(row['x'])\n        landmark_y = int(row['y'])\n        \n        # マスクを作成\n        mask, roi_coords = create_landmark_mask(\n            first_frame.shape, landmark_x, landmark_y, PATCH_SIZE\n        )\n        \n        masks_dict[landmark_id] = {\n            'mask': mask,\n            'roi_coords': roi_coords,\n            'center': (landmark_x, landmark_y)\n        }\n        \n        print(f\"  ランドマーク {landmark_id}: 中心({landmark_x}, {landmark_y}), ROI{roi_coords}\")\n    \n    # === マスクの可視化画像を保存 ===\n    vis_frame = visualize_landmark_masks(\n        first_frame, roi_df, PATCH_SIZE, LANDMARK_ID_NAME_MAP\n    )\n    \n    vis_output_path = os.path.join(savedir, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_masks_visualization.png\")\n    cv2.imwrite(vis_output_path, vis_frame)\n    print(f\"\\nマスク可視化画像を保存: {vis_output_path}\")\n    \n    # === 各ランドマークのマスク画像を個別に保存 ===\n    for landmark_id, mask_info in masks_dict.items():\n        mask = mask_info['mask']\n        mask_output_path = os.path.join(savedir, f\"{dataName}_landmark_{landmark_id}_mask.png\")\n        cv2.imwrite(mask_output_path, mask)\n    \n    print(f\"個別マスク画像を保存: {len(masks_dict)}個\")\n    \n    # === マスク情報をCSVに保存 ===\n    mask_info_list = []\n    for landmark_id, mask_info in masks_dict.items():\n        left, top, right, bottom = mask_info['roi_coords']\n        center_x, center_y = mask_info['center']\n        \n        mask_info_list.append({\n            'landmark_id': landmark_id,\n            'center_x': center_x,\n            'center_y': center_y,\n            'roi_left': left,\n            'roi_top': top,\n            'roi_right': right,\n            'roi_bottom': bottom,\n            'patch_size': PATCH_SIZE,\n            'roi_width': right - left,\n            'roi_height': bottom - top\n        })\n    \n    mask_info_df = pd.DataFrame(mask_info_list)\n    mask_info_csv_path = os.path.join(savedir, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_mask_info.csv\")\n    mask_info_df.to_csv(mask_info_csv_path, index=False)\n    print(f\"マスク情報をCSVに保存: {mask_info_csv_path}\")\n    \n    print(f'\\n{\"=\"*60}')\n    print(f'{dataName} のマスク作成完了')\n    print(f'{\"=\"*60}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uaj1ctmx5lr",
   "source": "### 窓ごとにマスク適用した信号を抽出",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cd93eunt2rn",
   "source": "for movie_idx in range(len(movie_paths)):\n    inputMoviePath = movie_paths[movie_idx]\n    rootDir = data_dirs[movie_idx]\n    dataName = movie_names[movie_idx]\n    \n    print(f'\\n{\"=\"*60}')\n    print(f'窓ごとのマスク適用信号抽出: {dataName}')\n    print(f'{\"=\"*60}')\n    \n    cap = cv2.VideoCapture(inputMoviePath)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    savedir = os.path.join(rootDir, SAVE_DIR)\n    os.makedirs(savedir, exist_ok=True)\n    \n    # マスク情報を読み込み\n    mask_info_csv_path = os.path.join(savedir, f\"{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_mask_info.csv\")\n    mask_info_df = pd.read_csv(mask_info_csv_path)\n    \n    # 窓のセグメント情報を作成\n    stride_segment_calculator = StrideSegmentCalculator(window_sizes=WINDOW_SIZES, strides=STRIDES)\n    analysis_df = stride_segment_calculator.create_analysis_dataframe(total_frames, fps)\n    \n    # 結果を格納するリスト\n    all_window_masked_results = []\n    \n    # 各窓ごとに処理\n    for idx, window_row in analysis_df.iterrows():\n        window_size = window_row['window_size']\n        frame_start = window_row['frame_start']\n        frame_end = window_row['frame_end']\n        stride = window_row['stride']\n        \n        # 窓の時間範囲を計算\n        window_start_time = frame_start / fps\n        window_end_time = frame_end / fps\n        \n        if window_end_time > total_frames / fps:\n            continue\n        \n        print(f\"\\n窓 {idx}: サイズ {window_size}s, ストライド {stride}s, フレーム {frame_start}-{frame_end}\")\n        \n        # 各ランドマークごとに処理\n        for mask_idx, mask_row in mask_info_df.iterrows():\n            landmark_id = int(mask_row['landmark_id'])\n            roi_left = int(mask_row['roi_left'])\n            roi_top = int(mask_row['roi_top'])\n            roi_right = int(mask_row['roi_right'])\n            roi_bottom = int(mask_row['roi_bottom'])\n            \n            # マスク画像を読み込み\n            mask_path = os.path.join(savedir, f\"{dataName}_landmark_{landmark_id}_mask.png\")\n            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n            \n            if mask is None:\n                print(f\"  警告: マスク画像が読み込めません: {mask_path}\")\n                continue\n            \n            # 窓範囲のフレームを読み込んでマスク適用\n            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n            \n            frame_rgb_means = []\n            frame_rgb_stds = []\n            frame_hsv_means = []\n            frame_hsv_stds = []\n            frame_lab_means = []\n            frame_lab_stds = []\n            frame_timestamps = []\n            \n            for frame_idx in range(int(frame_start), int(frame_end)):\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                timestamp = frame_idx / fps\n                \n                # BGRからRGBに変換\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                \n                # マスク領域のピクセルのみを抽出\n                masked_pixels_rgb = frame_rgb[mask > 0]\n                \n                if len(masked_pixels_rgb) > 0:\n                    # RGB統計量\n                    r_mean = np.mean(masked_pixels_rgb[:, 0])\n                    g_mean = np.mean(masked_pixels_rgb[:, 1])\n                    b_mean = np.mean(masked_pixels_rgb[:, 2])\n                    \n                    r_std = np.std(masked_pixels_rgb[:, 0])\n                    g_std = np.std(masked_pixels_rgb[:, 1])\n                    b_std = np.std(masked_pixels_rgb[:, 2])\n                    \n                    # HSV統計量\n                    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n                    masked_pixels_hsv = frame_hsv[mask > 0]\n                    \n                    h_mean = np.mean(masked_pixels_hsv[:, 0])\n                    s_mean = np.mean(masked_pixels_hsv[:, 1])\n                    v_mean = np.mean(masked_pixels_hsv[:, 2])\n                    \n                    h_std = np.std(masked_pixels_hsv[:, 0])\n                    s_std = np.std(masked_pixels_hsv[:, 1])\n                    v_std = np.std(masked_pixels_hsv[:, 2])\n                    \n                    # LAB統計量\n                    frame_lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n                    masked_pixels_lab = frame_lab[mask > 0]\n                    \n                    l_mean = np.mean(masked_pixels_lab[:, 0])\n                    a_mean = np.mean(masked_pixels_lab[:, 1])\n                    b_lab_mean = np.mean(masked_pixels_lab[:, 2])\n                    \n                    l_std = np.std(masked_pixels_lab[:, 0])\n                    a_std = np.std(masked_pixels_lab[:, 1])\n                    b_lab_std = np.std(masked_pixels_lab[:, 2])\n                    \n                    frame_rgb_means.append([r_mean, g_mean, b_mean])\n                    frame_rgb_stds.append([r_std, g_std, b_std])\n                    frame_hsv_means.append([h_mean, s_mean, v_mean])\n                    frame_hsv_stds.append([h_std, s_std, v_std])\n                    frame_lab_means.append([l_mean, a_mean, b_lab_mean])\n                    frame_lab_stds.append([l_std, a_std, b_lab_std])\n                    frame_timestamps.append(timestamp)\n            \n            # 窓内の統計量を計算\n            if len(frame_rgb_means) > 0:\n                frame_rgb_means = np.array(frame_rgb_means)\n                frame_rgb_stds = np.array(frame_rgb_stds)\n                frame_hsv_means = np.array(frame_hsv_means)\n                frame_hsv_stds = np.array(frame_hsv_stds)\n                frame_lab_means = np.array(frame_lab_means)\n                frame_lab_stds = np.array(frame_lab_stds)\n                \n                # 皮膚色角度を計算\n                skin_angle = calculate_skin_angle(\n                    frame_rgb_means[:, 0],\n                    frame_rgb_means[:, 1],\n                    frame_rgb_means[:, 2]\n                )\n                \n                window_info = {\n                    'landmark_id': landmark_id,\n                    'window_index': idx,\n                    'window_size': window_size,\n                    'stride': stride,\n                    'frame_start': frame_start,\n                    'frame_end': frame_end,\n                    'window_start_time': window_start_time,\n                    'window_end_time': window_end_time,\n                    'num_frames': len(frame_rgb_means),\n                    'num_masked_pixels': len(masked_pixels_rgb),\n                    'r_signal_masked': array_to_full_string(frame_rgb_means[:, 0]),\n                    'g_signal_masked': array_to_full_string(frame_rgb_means[:, 1]),\n                    'b_signal_masked': array_to_full_string(frame_rgb_means[:, 2]),\n                    'r_std_masked': array_to_full_string(frame_rgb_stds[:, 0]),\n                    'g_std_masked': array_to_full_string(frame_rgb_stds[:, 1]),\n                    'b_std_masked': array_to_full_string(frame_rgb_stds[:, 2]),\n                    'h_signal_masked': array_to_full_string(frame_hsv_means[:, 0]),\n                    's_signal_masked': array_to_full_string(frame_hsv_means[:, 1]),\n                    'v_signal_masked': array_to_full_string(frame_hsv_means[:, 2]),\n                    'h_std_masked': array_to_full_string(frame_hsv_stds[:, 0]),\n                    's_std_masked': array_to_full_string(frame_hsv_stds[:, 1]),\n                    'v_std_masked': array_to_full_string(frame_hsv_stds[:, 2]),\n                    'l_signal_masked': array_to_full_string(frame_lab_means[:, 0]),\n                    'a_signal_masked': array_to_full_string(frame_lab_means[:, 1]),\n                    'b_lab_signal_masked': array_to_full_string(frame_lab_means[:, 2]),\n                    'l_std_masked': array_to_full_string(frame_lab_stds[:, 0]),\n                    'a_std_masked': array_to_full_string(frame_lab_stds[:, 1]),\n                    'b_lab_std_masked': array_to_full_string(frame_lab_stds[:, 2]),\n                    'skin_angle': skin_angle\n                }\n                \n                all_window_masked_results.append(window_info)\n    \n    cap.release()\n    \n    # 結果をDataFrameに変換して保存\n    results_df = pd.DataFrame(all_window_masked_results)\n    results_csv_path = os.path.join(savedir, f'{dataName}_{PATCH_SIZE}x{PATCH_SIZE}_window_masked_signals.csv')\n    results_df.to_csv(results_csv_path, index=False, encoding='utf-8-sig')\n    print(f\"\\n窓ごとのマスク適用信号をCSVに保存: {results_csv_path}\")\n    print(f\"総窓数×ランドマーク数: {len(results_df)}\")\n    \n    print(f'\\n{\"=\"*60}')\n    print(f'{dataName} のマスク適用信号抽出完了')\n    print(f'{\"=\"*60}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvhr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}